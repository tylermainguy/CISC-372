{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":["from keras.callbacks import ModelCheckpoint\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import Lambda\n","from keras.layers import LSTM\n","from keras.models import Sequential\n","from keras.utils import np_utils\n","\n","import numpy as np\n","import os\n","import pandas as pd\n","import string\n","import sys\n","import tensorflow as tf\n","import time"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Read the dataset in\n","data = pd.read_csv(\"../input/kickstarter-project-statistics/most_backed.csv\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# The stupid punctuation that people included in their blurbs to be removed\n","punctuation = string.punctuation + \"\\xa0\" + \"\\x03\" + \"\\n\" + \"£©«®°²³´¹º»½Ç×Üàáäåæèéïöøüēπφ​‒–—‘’“”•…℃™Ⓡ◡★♥　ヒロー世浮絵️！\""],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# function to clean the strings from the blurbs\n","def clean_strings(str_list):\n","    cleaned = [entry.strip() for entry in str_list]\n","    # want to append both the name and the main category\n","    table = str.maketrans(\"\", \"\", punctuation) \n","    # Remove all the special characters mentioned above\n","    cleaned = [entry.translate(table) for entry in cleaned]\n","    # Set each of the characters remaining to lowercase\n","    cleaned = [entry.lower() for entry in cleaned]\n","    return cleaned\n","\n","# Clean the text data for blurbs, product title and product category\n","cleaned_blurb = clean_strings(data[\"blurb\"])\n","cleaned_name = clean_strings(data[\"title\"])\n","cleaned_category = clean_strings(data[\"category\"])"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Combine these lists together so each string contains the blurb, title and category of a pitch\n","new_list = [cleaned_name[i] + \" \" + cleaned_category[i] + \" \"  for i in range(len(cleaned_name))]"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Create one long string of product pitches\n","joined_text = \" \".join(new_list)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# now, we need a mapping from the individual characters to integers\n","chars = sorted(list(set(joined_text)))\n","char_to_int = dict((c,i) for i, c in enumerate(chars))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# count the length of the joined text, along with the unique characters (used below)\n","num_ch = len(joined_text)\n","num_ch_unique = len(chars)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Creating the training set of sequences of 50 characters\n","seq_length = 50\n","dataX = []\n","dataY = []\n","for i in range(0, num_ch - seq_length, 1):\n","    # in sequence is the next 50 chars\n","    seq_in = joined_text[i:i + seq_length]\n","    # prediction value is the character after the sequence\n","    seq_out = joined_text[i + seq_length]\n","    # add the new sequence to the dataset\n","    dataX.append([char_to_int[ch] for ch in seq_in])\n","    dataY.append(char_to_int[seq_out])"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# how many sequences do we have to analyze\n","n_patterns = len(dataX)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Put it in a form that's friendly to keras\n","X = np.reshape(dataX, (n_patterns, seq_length, 1))\n","\n","# Normalize the data so it performs well in the sigmoid transformation\n","X = X / float(num_ch_unique)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# convert the y data to categorical values, for predictions\n","y = np_utils.to_categorical(dataY)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Now, time for the model!\n","model = Sequential()\n","# These hyperparam selections are more arbitrary than anything\n","model.add(LSTM(256,input_shape=(X.shape[1], X.shape[2]),return_sequences=True))\n","model.add(Dropout(0.3))\n","model.add(LSTM(256,return_sequences=True))\n","model.add(Dropout(0.3))\n","model.add(LSTM(256))\n","model.add(Dropout(0.3))\n","# Apply the temperature reduction\n","model.add(Lambda(lambda x: x / 2))\n","# softmax activation function\n","model.add(Dense(y.shape[1], activation=\"softmax\"))\n","# adam optimizer for speed\n","model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Sequential' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-97e64c6ccfdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now, time for the model!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# These hyperparam selections are more arbitrary than anything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["# define checkpoints so that we can choose the best set of weights for the model (and so I don't loose everything when the kernel crashes)\n","filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n","callbacks_list = [checkpoint]"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["model.fit(X, y, epochs=98, batch_size=64, callbacks=callbacks_list)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Commented out, but can be used to load in the best weight model to date instead of retraining\n","# fname = \"../input/weights-updated/weights-improvement-41-1.6265.hdf5\"\n","# model.load_weights(fname)\n","# model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# pick a random seed\n","start = np.random.randint(0, len(dataX)-1)\n","# This is a random sample to start on, and see what the model generates\n","pattern = dataX[start]\n","print(\"Seed:\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n","# generate characters\n","for i in range(1000):\n","    x = np.reshape(pattern, (1, len(pattern), 1))\n","    x = x / float(num_ch_unique)\n","    prediction = model.predict(x, verbose=0)\n","    index = np.argmax(prediction)\n","    result = int_to_char[index]\n","    seq_in = [int_to_char[value] for value in pattern]\n","    sys.stdout.write(result)\n","    pattern.append(index)\n","    pattern = pattern[1:len(pattern)]\n","print(\"\\nDone.\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":[],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.4 64-bit","language":"python","name":"python37464bitffa1aa5c80444031bbf1593eb8fa54f9"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4-final"}},"nbformat":4,"nbformat_minor":4}