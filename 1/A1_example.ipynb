{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTs11WubmRhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download data (-q is the quiet mode)\n",
        "! wget -q https://www.dropbox.com/s/lhb1awpi769bfdr/test.csv?dl=1 -O test.csv\n",
        "! wget -q https://www.dropbox.com/s/gudb5eunj700s7j/train.csv?dl=1 -O train.csv\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrsga6qkouO1",
        "colab_type": "code",
        "outputId": "c7fc42d7-9a61-4f64-d278-06f741694a5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "# Import the pandas library for useful data structures \n",
        "# and functionality for data analysis\n",
        "import pandas as pd\n",
        "\n",
        "# Read in the training set\n",
        "Xy_train = pd.read_csv('train.csv', engine='python')\n",
        "# Remove the target column from the data set for training\n",
        "# (that's the column we're predicting, can't be in the training set)\n",
        "X_train = Xy_train.drop(columns=['price_rating'])\n",
        "# Extract the target column from the dataset\n",
        "y_train = Xy_train[['price_rating']]\n",
        "\n",
        "# Prints the number of entries in the training set\n",
        "print('traning', len(X_train))\n",
        "# Histogram plot of the different classes in our target attribute\n",
        "Xy_train.price_rating.hist()\n",
        "\n",
        "# Read in the test set\n",
        "X_test = pd.read_csv('test.csv', engine='python')\n",
        "\n",
        "# Extract the unique ID for each sample from the testing dataset\n",
        "# (this is used for submissions to kaggle)\n",
        "testing_ids = X_test.Id\n",
        "# Output the number of samples in the test set\n",
        "print('testing', len(X_test))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "traning 7631\n",
            "testing 7632\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJCTrEGvuJgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vFDJ3lNmE6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model training and tuning\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "\n",
        "# Set random generator to produce consistent random values\n",
        "np.random.seed(0)\n",
        "\n",
        "# Identify the numeric features in the dataset used in the model\n",
        "numeric_features = [\"latitude\", \"longitude\", \"accommodates\", \"bathrooms\", \n",
        "                    \"bedrooms\", \"beds\", \"square_feet\",\"guests_included\", \n",
        "                    \"number_of_reviews\", \"review_scores_accuracy\", \n",
        "                    \"review_scores_cleanliness\", \"review_scores_checkin\", \n",
        "                    \"review_scores_communication\", \"review_scores_location\", \n",
        "                    \"review_scores_value\", \"reviews_per_month\"]\n",
        "\n",
        "# Simple imputer that replaces NaN values with the median of all other existing\n",
        "# values in the column. Scaler takes feature values and scales them to unit \n",
        "# variance and mean of 0. Added to a Pipeline object to be used later\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Identify the categorical features to be used in the model\n",
        "categorical_features = [\"host_is_superhost\", \"property_type\",\n",
        "                        \"bed_type\", \"is_business_travel_ready\"]\n",
        "\n",
        "# Apply impute (as described above) with default value \"missing\". Then performs\n",
        "# one hot encoding, which takes possible categorical feature values and splits\n",
        "# them into columns with either 1 or 0 (present or not present). If we were\n",
        "# to just use label encoding, then our model would infer that the higher\n",
        "# the categorical value the better (which is certainly not the case) \n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Apply relevant pipeline operations to each of the column types (i.e. apply\n",
        "# the categorical_transformer to the list of categorical columns, ...)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Regression pipeline for training the XGBoost ensemble. First step of the \n",
        "# pipeline is to apply the preprocessing steps, then to train the regression\n",
        "# whose objective function to optimize is softmax multi: means do multi-class\n",
        "# classification\n",
        "regr = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('regressor', XGBClassifier(\n",
        "                          objective='multi:softmax', seed=1))])\n",
        "\n",
        "# Syntax to unpack the column values (*numeric_features). Takes the list of\n",
        "# column values and turns them into an argument list to index data from our\n",
        "# X_train and X_test data sets (take only the columns we want)\n",
        "X_train = X_train[[*numeric_features, *categorical_features]]\n",
        "X_test = X_test[[*numeric_features, *categorical_features]]\n",
        "\n",
        "# `__` denotes attribute \n",
        "# (e.g. regressor__n_estimators means the `n_estimators` param for `regressor`\n",
        "#  which is our xgb). Passing in list give the possible values to be checked\n",
        "#  in an exhaustive search.\n",
        "param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['mean'],\n",
        "    'regressor__n_estimators': [50, 100, 150, 200],\n",
        "    'regressor__max_depth':[10, 15, 20, 25, 30],\n",
        "}\n",
        "\n",
        "# Grid search performs an exhaustive search of the specified parameters from\n",
        "# param_grid on a given model. Performed on the XGBoost model (regr) that we \n",
        "# specified above. Performs 5-fold cross validation, scores function based\n",
        "# on the accuracy (f-score), and runs 2 jobs in parallel to complete the task.\n",
        "# After setting up model, performs fit to run parameter optimization for the \n",
        "# model. Verbose 3 just means there's a verbose output when running (the \n",
        "# higher the parameter value, the more messages)\n",
        "grid_search = GridSearchCV(\n",
        "    regr, param_grid, cv=5, verbose=3, n_jobs=2, \n",
        "    scoring='accuracy')\n",
        "\n",
        "# Parameter optimization and model fitting\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Output the best accuracy from the model fit on training\n",
        "print('best score {}'.format(grid_search.best_score_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF6WrzdKmJ97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction & generating the submission file\n",
        "y_pred = grid_search.predict(X_test)\n",
        "pd.DataFrame(\n",
        "    {'Id': testing_ids, 'price_rating':y_pred}).to_csv('sample_submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}