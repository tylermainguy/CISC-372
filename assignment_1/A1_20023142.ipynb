{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_20023142.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpp9nVzD4UpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import logging\n",
        "import math\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from lightgbm.sklearn import LGBMClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n8uitdh4WFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget -q https://www.dropbox.com/s/lhb1awpi769bfdr/test.csv?dl=1 -O test.csv\n",
        "! wget -q https://www.dropbox.com/s/gudb5eunj700s7j/train.csv?dl=1 -O train.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pB_6uvfJVhy",
        "colab_type": "text"
      },
      "source": [
        "# Read in the Dataset\n",
        "\n",
        "First, we're just going to read in the dataset. We'll seperate the features from the target attribute, which in this dataset is **the price rating** of the individual listing. We'll output the distribution of labels in the training set to get an idea as to how our data is distributed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LFB5bQu4ZZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in the given datasets\n",
        "Xy_train = pd.read_csv(\"train.csv\", engine=\"python\")\n",
        "X_test = pd.read_csv(\"test.csv\", engine=\"python\")\n",
        "\n",
        "# Seperate features from target in training set\n",
        "X_train = Xy_train.drop(columns=[\"price_rating\"])\n",
        "y_train = Xy_train[\"price_rating\"]\n",
        "\n",
        "# Output number of training and test samples\n",
        "print(\"Number of training samples: {}\".format(len(X_train)))\n",
        "print(\"Number of testing samples: {}\".format(len(X_test)))\n",
        "\n",
        "testing_ids = X_test.Id\n",
        "# Histogram of price_rating quantities in the training set. \n",
        "Xy_train.price_rating.hist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pxb-zIwJqQd",
        "colab_type": "text"
      },
      "source": [
        "# Understand Our Data\n",
        "\n",
        "The next step is to get an intuitive idea as to which attributes may be useful when building our model. We can view the statistical metrics for each numerical feature in our dataset in order to understand how they're distributed, along with getting a sense at which data might actually prove useful (i.e. if there are a lot of missing values, we may want to avoid a column).\n",
        "\n",
        "For categorical data, we can't quite do this same operation. I'm going to select a subset of those features that I believe may have some predictive properties. This is a purely intuitive approach, but viewing the data gives us a clear sense of what we want to exclude (things like URLs, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMuYPZF9nEAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train.info()\n",
        "X_train.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCD3rQ694uOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Give us an idea of the non-numerical (mostly categorical data) we may want to use\n",
        "X_train.select_dtypes(include=[\"object\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrGxdLKONnOK",
        "colab_type": "text"
      },
      "source": [
        "# Clean the Data\n",
        "\n",
        "The next step is to take the selected attributes and clean them up. For numerical data, this means we're going to replace the missing values (NaN) with the mean of the column, then standardize the data (0 mean with standard deviation 1). \n",
        "\n",
        "For categorical data, we'll replace missing values with an indicative **\"missing_value\"** tag that our model will use. We'll also \"one-hot encode\" the data, which is essentially splitting all of the potential values for a category into their own columns, with 1 representing it being present in a data, and 0 representing it's absence. This is done to avoid attributing scores to categorical data (i.e. instead of 1,2,3,4,...), as the model will learn that higher values of categorical values contain more meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "relgr5R54v0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "# Select numerical attributes to be used\n",
        "numeric_features = [\"latitude\", \"longitude\", \"accommodates\", \"bathrooms\", \n",
        "                    \"bedrooms\", \"beds\", \"square_feet\",\"guests_included\", \n",
        "                    \"number_of_reviews\", \"review_scores_accuracy\", \n",
        "                    \"review_scores_cleanliness\", \"review_scores_checkin\", \n",
        "                    \"review_scores_communication\", \"review_scores_location\", \n",
        "                    \"review_scores_value\", \"reviews_per_month\",\"availability_365\",\n",
        "                    \"availability_90\", \"availability_30\",\n",
        "                    \"availability_60\", \"number_of_reviews_ltm\",\n",
        "                    \"minimum_nights\",\"maximum_nights\",\n",
        "                    \"minimum_nights_avg_ntm\", \"maximum_minimum_nights\"]\n",
        "\n",
        "# Impute and scale numerical data\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "                (\"imputer\", SimpleImputer(fill_value=\"median\")),\n",
        "                (\"scaler\", StandardScaler())\n",
        "\n",
        "])\n",
        "\n",
        "# Select categorical features\n",
        "categorical_features = [\"host_is_superhost\", \"property_type\",\n",
        "                        \"bed_type\", \"is_business_travel_ready\",\n",
        "                        \"property_type\", \"host_is_superhost\",\n",
        "                        \"room_type\", \"bed_type\", \"cancellation_policy\"]\n",
        "\n",
        "# Select categorical transformer\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "                (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
        "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "# Apply the transformers to the columns of the dataset\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "          (\"num\", numeric_transformer, numeric_features),\n",
        "          (\"cat\", categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply relevant pipeline operations to each of the column types\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uduDc-IBnFDJ",
        "colab_type": "text"
      },
      "source": [
        "# Classification Using LightGBM\n",
        "\n",
        "The first model that was used to fit the data was LightGBM. With small-medium structured data, ensemble tree methods tend to perform quite well, so using LightGBM made logical sense. Parameter grid is used to exhaustively search the hyperparameter configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2wGPDNU4xyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build classification pipeline for LightGBM\n",
        "regr = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('regressor', LGBMClassifier(boosting_type='gbdt',\n",
        "                        objective='multiclass',\n",
        "                        num_classes=3,\n",
        "                      ))])\n",
        "\n",
        "\n",
        "# Select features from sets to be used\n",
        "X_train = X_train[[*numeric_features, *categorical_features]]\n",
        "X_test = X_test[[*numeric_features, *categorical_features]]\n",
        "\n",
        "# `__` denotes attribute \n",
        "# (e.g. regressor__n_estimators means the `n_estimators` param for `regressor`\n",
        "#  which is our xgb).\n",
        "param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['median'],\n",
        "    'regressor__num_leaves': [30, 45],\n",
        "    'regressor__n_estimators': [200, 300],\n",
        "    'regressor__max_depth': [10, 15],\n",
        "    'regressor__learning_rate': [0.03, 0.05],\n",
        "    'regressor__min_data_in_leaf' : [10, 20],\n",
        "}\n",
        "\n",
        "# Grid search the parameters above, 3-fold cv\n",
        "grid_search = GridSearchCV(\n",
        "    regr, param_grid, cv=3, verbose=3, n_jobs=1, \n",
        "    scoring='accuracy')\n",
        "\n",
        "# Parameter optimization and model fitting\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Output the best accuracy from the model fit on training\n",
        "print('best score {}'.format(grid_search.best_score_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfbfPBzn40sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get test set prediction and convert it into a cvb\n",
        "y_pred = grid_search.predict(X_test)\n",
        "pd.DataFrame(\n",
        "    {'Id': testing_ids, 'price_rating':y_pred}).to_csv(r'./mainguy_submission_three.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfa0T4Ow43Ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount and save to drive\n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qw4lewA44j0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy submission CSV to drive (needed when kaggle command not working)\n",
        "!cp mainguy_submission_three.csv \"drive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}